{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac14c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "\n",
    "from train import train \n",
    "from test import test\n",
    "from test_beam import test_beam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270cd592",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24cdd0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreFalseAction(option_strings=['--no-attention'], dest='attention', nargs=0, const=False, default=True, type=None, choices=None, help='Use this for convcap without attention', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Convolutional Image Captioning Model')\n",
    "\n",
    "parser.add_argument('--model_dir', help='output directory to save models & results')\n",
    "\n",
    "parser.add_argument('-g', '--gpu', type=int, default=0,\\\n",
    "                    help='gpu device id')\n",
    "\n",
    "parser.add_argument('--coco_root', type=str, default= './data/coco/',\\\n",
    "                    help='directory containing coco dataset train2014, val2014, & annotations')\n",
    "\n",
    "parser.add_argument('-t', '--is_train', type=int, default=1,\\\n",
    "                    help='use 1 to train model')\n",
    "\n",
    "parser.add_argument('-e', '--epochs', type=int, default=30,\\\n",
    "                    help='number of training epochs')\n",
    "\n",
    "parser.add_argument('-b', '--batchsize', type=int, default=20,\\\n",
    "                    help='number of images per training batch')\n",
    "\n",
    "parser.add_argument('-c', '--ncap_per_img', type=int, default=5,\\\n",
    "                    help='ground-truth captions per image in training batch')\n",
    "\n",
    "parser.add_argument('-n', '--num_layers', type=int, default=3,\\\n",
    "                    help='depth of convcap network')\n",
    "\n",
    "parser.add_argument('-m', '--nthreads', type=int, default=4,\\\n",
    "                    help='pytorch data loader threads')\n",
    "\n",
    "# parser.add_argument('-ft', '--finetune_after', type=int, default=8,\\\n",
    "#                     help='epochs after which vgg16 is fine-tuned')\n",
    "\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=5e-5,\\\n",
    "                    help='learning rate for convcap')\n",
    "\n",
    "parser.add_argument('-st', '--lr_step_size', type=int, default=15,\\\n",
    "                    help='epochs to decay learning rate after')\n",
    "\n",
    "parser.add_argument('-sc', '--score_select', type=str, default='CIDEr',\\\n",
    "                    help='metric to pick best model')\n",
    "\n",
    "parser.add_argument('--beam_size', type=int, default=1, \\\n",
    "                    help='beam size to use for test') \n",
    "\n",
    "parser.add_argument('--attention', dest='attention', action='store_true', \\\n",
    "                    help='Use this for convcap with attention (by default set)')\n",
    "\n",
    "parser.add_argument('--no-attention', dest='attention', action='store_false', \\\n",
    "                    help='Use this for convcap without attention')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf0548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.set_defaults(attention=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aeed33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.finetune_after = 8\n",
    "args.model_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b1870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f87f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import numpy as np \n",
    "import json\n",
    "import time\n",
    " \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models                                                                     \n",
    "\n",
    "from coco_loader import coco_loader\n",
    "from convcap import convcap\n",
    "from vggfeats import Vgg16Feats\n",
    "from tqdm import tqdm \n",
    "from test import test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92131ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d80637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "if (args.is_train == 1):\n",
    "    print('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d858dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotation file...\n",
      "Found 113287 images in split: train\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading train data ... 5.094635 secs\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "train_data = coco_loader(args.coco_root, split='train', ncap_per_img=args.ncap_per_img)\n",
    "print('[DEBUG] Loading train data ... %f secs' % (time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset=train_data, num_workers=0, batch_size=args.batchsize, \\\n",
    "                               shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238f4f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16Feats(\n",
       "  (features_nopool): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "  )\n",
       "  (features_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imgcnn = Vgg16Feats()\n",
    "model_imgcnn.cuda()\n",
    "model_imgcnn.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1045204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convcap(\n",
       "  (emb_0): Embedding(9221, 512, padding_idx=0)\n",
       "  (emb_1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (imgproj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (resproj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (1): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (2): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "  )\n",
       "  (attention): ModuleList(\n",
       "    (0): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_0): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (classifier_1): Linear(in_features=256, out_features=9221, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convcap model\n",
    "model_convcap = convcap(train_data.numwords, args.num_layers, is_attention=args.attention)\n",
    "model_convcap.cuda()\n",
    "model_convcap.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ce59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model_convcap.parameters(), lr=args.learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=.1)\n",
    "img_optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3353b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = args.batchsize\n",
    "ncap_per_img = args.ncap_per_img\n",
    "batchsize_cap = batchsize*ncap_per_img\n",
    "max_tokens = train_data.max_tokens\n",
    "nbatches = np.int_(np.floor((len(train_data.ids)*1.)/batchsize)) \n",
    "bestscore = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1e0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_img_per_cap(imgsfeats, imgsfc7, ncap_per_img):\n",
    "    batchsize, featdim, feat_h, feat_w = imgsfeats.size()\n",
    "    batchsize_cap = batchsize*ncap_per_img\n",
    "    imgsfeats = imgsfeats.unsqueeze(1).expand(batchsize, ncap_per_img, featdim, feat_h, feat_w)\n",
    "    imgsfeats = imgsfeats.contiguous().view(batchsize_cap, featdim, feat_h, feat_w)\n",
    "    \n",
    "    batchsize, featdim = imgsfc7.size()\n",
    "    batchsize_cap = batchsize*ncap_per_img\n",
    "    imgsfc7 = imgsfc7.unsqueeze(1).expand(batchsize, ncap_per_img, featdim)\n",
    "    imgsfc7 = imgsfc7.contiguous().view(batchsize_cap, featdim)\n",
    "    \n",
    "    return imgsfeats, imgsfc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b32322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e9995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94edb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad88204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   for epoch in range(args.epochs):\n",
    "# 코드가 잘 돌아가는지 확인하기 위해 2번만 돌려봤습니다. 전 30(args.epochs)번 돌렸습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13bd78c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/s21110373/.conda/envs/convcap2/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:09<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 0 has loss 7.463197\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.021567 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1165853.60 tokens per second.\n",
      "PTBTokenizer tokenized 47557 tokens at 397720.00 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
      "Progress: 384.5M / 384.5M (100.0%)\n",
      "Extracting stanford-corenlp-3.6.0 ...\n",
      "Done.\n",
      "computing Bleu score...\n",
      "{'testlen': 42558, 'reflen': 44383, 'guess': [42558, 37558, 32558, 27558], 'correct': [24410, 8725, 2180, 491]}\n",
      "ratio: 0.9588806525020626\n",
      "Bleu_1: 0.549\n",
      "Bleu_2: 0.350\n",
      "Bleu_3: 0.199\n",
      "Bleu_4: 0.108\n",
      "computing METEOR score...\n",
      "METEOR: 0.148\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.410\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.299\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Threads( StanfordCoreNLP ) [30.663 seconds]\n",
      "Threads( StanfordCoreNLP ) [25.77 seconds]\n",
      "Threads( StanfordCoreNLP ) [12.270 seconds]\n",
      "Parsing test captions\n",
      "Threads( StanfordCoreNLP ) [8.567 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.527 min\n",
      "SPICE: 0.083\n",
      "[DEBUG] Saving model at epoch 0 with CIDEr score of 0.298641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:22<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 1 has loss 4.224917\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.312886 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:48<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1215924.12 tokens per second.\n",
      "PTBTokenizer tokenized 51014 tokens at 411884.71 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 46015, 'reflen': 46404, 'guess': [46015, 41015, 36015, 31015], 'correct': [29327, 12956, 4582, 1468]}\n",
      "ratio: 0.9916171019739465\n",
      "Bleu_1: 0.632\n",
      "Bleu_2: 0.445\n",
      "Bleu_3: 0.292\n",
      "Bleu_4: 0.185\n",
      "computing METEOR score...\n",
      "METEOR: 0.189\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.459\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.532\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [11.827 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 23.71 s\n",
      "SPICE: 0.117\n",
      "[DEBUG] Saving model at epoch 1 with CIDEr score of 0.532203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:09<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 2 has loss 3.785114\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.726461 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1219903.17 tokens per second.\n",
      "PTBTokenizer tokenized 51108 tokens at 409198.91 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 46109, 'reflen': 46400, 'guess': [46109, 41109, 36109, 31109], 'correct': [30320, 13938, 5227, 1808]}\n",
      "ratio: 0.9937284482758407\n",
      "Bleu_1: 0.653\n",
      "Bleu_2: 0.469\n",
      "Bleu_3: 0.316\n",
      "Bleu_4: 0.207\n",
      "computing METEOR score...\n",
      "METEOR: 0.199\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.474\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.603\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [9.607 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 21.32 s\n",
      "SPICE: 0.130\n",
      "[DEBUG] Saving model at epoch 2 with CIDEr score of 0.603230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:07<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 3 has loss 3.577887\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.414381 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1158365.65 tokens per second.\n",
      "PTBTokenizer tokenized 51667 tokens at 377254.51 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 46668, 'reflen': 46733, 'guess': [46668, 41668, 36668, 31668], 'correct': [30905, 14295, 5533, 2053]}\n",
      "ratio: 0.9986091198938438\n",
      "Bleu_1: 0.661\n",
      "Bleu_2: 0.476\n",
      "Bleu_3: 0.324\n",
      "Bleu_4: 0.217\n",
      "computing METEOR score...\n",
      "METEOR: 0.206\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.478\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.643\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [9.465 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 18.67 s\n",
      "SPICE: 0.136\n",
      "[DEBUG] Saving model at epoch 3 with CIDEr score of 0.643220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:11<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 4 has loss 3.449286\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.814329 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1103854.00 tokens per second.\n",
      "PTBTokenizer tokenized 52723 tokens at 405980.48 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47724, 'reflen': 47537, 'guess': [47724, 42724, 37724, 32724], 'correct': [31588, 14854, 5852, 2209]}\n",
      "ratio: 1.0039337778992994\n",
      "Bleu_1: 0.662\n",
      "Bleu_2: 0.480\n",
      "Bleu_3: 0.329\n",
      "Bleu_4: 0.222\n",
      "computing METEOR score...\n",
      "METEOR: 0.210\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.481\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.675\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [10.267 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 19.81 s\n",
      "SPICE: 0.141\n",
      "[DEBUG] Saving model at epoch 4 with CIDEr score of 0.674932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [42:58<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 5 has loss 3.358862\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.385154 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1186977.94 tokens per second.\n",
      "PTBTokenizer tokenized 53470 tokens at 396041.09 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48471, 'reflen': 47973, 'guess': [48471, 43471, 38471, 33471], 'correct': [32152, 15238, 6142, 2394]}\n",
      "ratio: 1.0103808392220415\n",
      "Bleu_1: 0.663\n",
      "Bleu_2: 0.482\n",
      "Bleu_3: 0.334\n",
      "Bleu_4: 0.227\n",
      "computing METEOR score...\n",
      "METEOR: 0.214\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.485\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.690\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [9.5 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 18.40 s\n",
      "SPICE: 0.144\n",
      "[DEBUG] Saving model at epoch 5 with CIDEr score of 0.690072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:22<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 6 has loss 3.290611\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.975393 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1188484.13 tokens per second.\n",
      "PTBTokenizer tokenized 52811 tokens at 429466.63 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47812, 'reflen': 47628, 'guess': [47812, 42812, 37812, 32812], 'correct': [32243, 15307, 6157, 2423]}\n",
      "ratio: 1.0038632737045226\n",
      "Bleu_1: 0.674\n",
      "Bleu_2: 0.491\n",
      "Bleu_3: 0.340\n",
      "Bleu_4: 0.232\n",
      "computing METEOR score...\n",
      "METEOR: 0.215\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.487\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.712\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [8.734 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 19.50 s\n",
      "SPICE: 0.147\n",
      "[DEBUG] Saving model at epoch 6 with CIDEr score of 0.712275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:19<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 7 has loss 3.237071\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.077819 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1149525.78 tokens per second.\n",
      "PTBTokenizer tokenized 52671 tokens at 381930.55 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47672, 'reflen': 47483, 'guess': [47672, 42672, 37672, 32672], 'correct': [31987, 15344, 6298, 2550]}\n",
      "ratio: 1.0039803719225617\n",
      "Bleu_1: 0.671\n",
      "Bleu_2: 0.491\n",
      "Bleu_3: 0.343\n",
      "Bleu_4: 0.237\n",
      "computing METEOR score...\n",
      "METEOR: 0.217\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.488\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.724\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [8.282 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.99 s\n",
      "SPICE: 0.147\n",
      "[DEBUG] Saving model at epoch 7 with CIDEr score of 0.723898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/s21110373/.conda/envs/convcap2/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:15<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 8 has loss 3.168301\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.131937 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1158153.98 tokens per second.\n",
      "PTBTokenizer tokenized 53063 tokens at 423867.93 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48064, 'reflen': 47782, 'guess': [48064, 43064, 38064, 33064], 'correct': [32769, 16056, 6788, 2849]}\n",
      "ratio: 1.0059018040266\n",
      "Bleu_1: 0.682\n",
      "Bleu_2: 0.504\n",
      "Bleu_3: 0.357\n",
      "Bleu_4: 0.250\n",
      "computing METEOR score...\n",
      "METEOR: 0.224\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.498\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.767\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [7.652 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 19.04 s\n",
      "SPICE: 0.155\n",
      "[DEBUG] Saving model at epoch 8 with CIDEr score of 0.767302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:15<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 9 has loss 3.089793\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 6.906132 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1172399.71 tokens per second.\n",
      "PTBTokenizer tokenized 53092 tokens at 376407.66 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48093, 'reflen': 47750, 'guess': [48093, 43093, 38093, 33093], 'correct': [33502, 16859, 7334, 3120]}\n",
      "ratio: 1.0071832460732775\n",
      "Bleu_1: 0.697\n",
      "Bleu_2: 0.522\n",
      "Bleu_3: 0.374\n",
      "Bleu_4: 0.265\n",
      "computing METEOR score...\n",
      "METEOR: 0.232\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.510\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.812\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [8.224 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.86 s\n",
      "SPICE: 0.160\n",
      "[DEBUG] Saving model at epoch 9 with CIDEr score of 0.812313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:22<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 10 has loss 3.030770\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 8.710794 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1174840.42 tokens per second.\n",
      "PTBTokenizer tokenized 52455 tokens at 422441.29 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47456, 'reflen': 47348, 'guess': [47456, 42456, 37456, 32456], 'correct': [33209, 16548, 7167, 3087]}\n",
      "ratio: 1.0022809833572486\n",
      "Bleu_1: 0.700\n",
      "Bleu_2: 0.522\n",
      "Bleu_3: 0.374\n",
      "Bleu_4: 0.265\n",
      "computing METEOR score...\n",
      "METEOR: 0.231\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.508\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.819\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [8.132 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.60 s\n",
      "SPICE: 0.162\n",
      "[DEBUG] Saving model at epoch 10 with CIDEr score of 0.819346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:18<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 11 has loss 2.976481\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.879016 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1252216.30 tokens per second.\n",
      "PTBTokenizer tokenized 52505 tokens at 435432.42 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47506, 'reflen': 47316, 'guess': [47506, 42506, 37506, 32506], 'correct': [33478, 16769, 7374, 3185]}\n",
      "ratio: 1.0040155549919478\n",
      "Bleu_1: 0.705\n",
      "Bleu_2: 0.527\n",
      "Bleu_3: 0.380\n",
      "Bleu_4: 0.271\n",
      "computing METEOR score...\n",
      "METEOR: 0.235\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.515\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.844\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [8.267 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.95 s\n",
      "SPICE: 0.163\n",
      "[DEBUG] Saving model at epoch 11 with CIDEr score of 0.843690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:22<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 12 has loss 2.921816\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.090016 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1196285.28 tokens per second.\n",
      "PTBTokenizer tokenized 53077 tokens at 420160.56 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48078, 'reflen': 47840, 'guess': [48078, 43078, 38078, 33078], 'correct': [33573, 17139, 7653, 3349]}\n",
      "ratio: 1.004974916387939\n",
      "Bleu_1: 0.698\n",
      "Bleu_2: 0.527\n",
      "Bleu_3: 0.382\n",
      "Bleu_4: 0.274\n",
      "computing METEOR score...\n",
      "METEOR: 0.236\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.514\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.849\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [7.900 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.35 s\n",
      "SPICE: 0.166\n",
      "[DEBUG] Saving model at epoch 12 with CIDEr score of 0.848544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:12<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 13 has loss 2.875390\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.126328 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1238889.47 tokens per second.\n",
      "PTBTokenizer tokenized 53107 tokens at 442052.28 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48108, 'reflen': 47817, 'guess': [48108, 43108, 38108, 33108], 'correct': [34059, 17254, 7695, 3354]}\n",
      "ratio: 1.0060857017378546\n",
      "Bleu_1: 0.708\n",
      "Bleu_2: 0.532\n",
      "Bleu_3: 0.385\n",
      "Bleu_4: 0.276\n",
      "computing METEOR score...\n",
      "METEOR: 0.239\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.518\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.865\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [7.48 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 16.72 s\n",
      "SPICE: 0.169\n",
      "[DEBUG] Saving model at epoch 13 with CIDEr score of 0.865306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:06<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 14 has loss 2.816106\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.149606 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:48<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1145159.47 tokens per second.\n",
      "PTBTokenizer tokenized 52976 tokens at 428391.25 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47977, 'reflen': 47679, 'guess': [47977, 42977, 37977, 32977], 'correct': [34200, 17602, 7925, 3525]}\n",
      "ratio: 1.006250131084943\n",
      "Bleu_1: 0.713\n",
      "Bleu_2: 0.540\n",
      "Bleu_3: 0.393\n",
      "Bleu_4: 0.284\n",
      "computing METEOR score...\n",
      "METEOR: 0.242\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.521\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.884\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [7.498 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.74 s\n",
      "SPICE: 0.170\n",
      "[DEBUG] Saving model at epoch 14 with CIDEr score of 0.884283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:09<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 15 has loss 2.796477\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.331100 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1181292.89 tokens per second.\n",
      "PTBTokenizer tokenized 53062 tokens at 419440.03 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48063, 'reflen': 47755, 'guess': [48063, 43063, 38063, 33063], 'correct': [34221, 17553, 7887, 3471]}\n",
      "ratio: 1.0064495864307192\n",
      "Bleu_1: 0.712\n",
      "Bleu_2: 0.539\n",
      "Bleu_3: 0.392\n",
      "Bleu_4: 0.282\n",
      "computing METEOR score...\n",
      "METEOR: 0.242\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.520\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.889\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [6.418 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.49 s\n",
      "SPICE: 0.171\n",
      "[DEBUG] Saving model at epoch 15 with CIDEr score of 0.889499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:14<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 16 has loss 2.779297\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 6.906588 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1188615.53 tokens per second.\n",
      "PTBTokenizer tokenized 53096 tokens at 427340.20 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48097, 'reflen': 47786, 'guess': [48097, 43097, 38097, 33097], 'correct': [34156, 17456, 7799, 3447]}\n",
      "ratio: 1.0065081823127904\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.536\n",
      "Bleu_3: 0.389\n",
      "Bleu_4: 0.280\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.519\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.877\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [5.546 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 16.57 s\n",
      "SPICE: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:23<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 17 has loss 2.763423\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.310069 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1173953.48 tokens per second.\n",
      "PTBTokenizer tokenized 53187 tokens at 433057.02 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48188, 'reflen': 47834, 'guess': [48188, 43188, 38188, 33188], 'correct': [34218, 17417, 7730, 3358]}\n",
      "ratio: 1.007400593719927\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.535\n",
      "Bleu_3: 0.387\n",
      "Bleu_4: 0.277\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.517\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.874\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [6.228 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.79 s\n",
      "SPICE: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:11<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 18 has loss 2.748347\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.795748 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1243634.77 tokens per second.\n",
      "PTBTokenizer tokenized 53077 tokens at 408674.53 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48078, 'reflen': 47779, 'guess': [48078, 43078, 38078, 33078], 'correct': [34146, 17437, 7803, 3453]}\n",
      "ratio: 1.0062579794470163\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.536\n",
      "Bleu_3: 0.389\n",
      "Bleu_4: 0.280\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.518\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.880\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [6.226 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 16.37 s\n",
      "SPICE: 0.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:07<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 19 has loss 2.733787\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 6.763798 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1235157.35 tokens per second.\n",
      "PTBTokenizer tokenized 53120 tokens at 385715.91 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48121, 'reflen': 47762, 'guess': [48121, 43121, 38121, 33121], 'correct': [34163, 17464, 7851, 3459]}\n",
      "ratio: 1.0075164356601272\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.536\n",
      "Bleu_3: 0.390\n",
      "Bleu_4: 0.280\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.518\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.877\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [6.19 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.76 s\n",
      "SPICE: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:05<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 20 has loss 2.719696\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.355611 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1110999.11 tokens per second.\n",
      "PTBTokenizer tokenized 53240 tokens at 398907.66 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48241, 'reflen': 47903, 'guess': [48241, 43241, 38241, 33241], 'correct': [34060, 17249, 7646, 3345]}\n",
      "ratio: 1.0070559255161262\n",
      "Bleu_1: 0.706\n",
      "Bleu_2: 0.531\n",
      "Bleu_3: 0.383\n",
      "Bleu_4: 0.274\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.516\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.872\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [6.839 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 16.01 s\n",
      "SPICE: 0.169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:34<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 21 has loss 2.706661\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.761214 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1180121.45 tokens per second.\n",
      "PTBTokenizer tokenized 53286 tokens at 402117.30 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48287, 'reflen': 47863, 'guess': [48287, 43287, 38287, 33287], 'correct': [34041, 17333, 7734, 3384]}\n",
      "ratio: 1.008858617303533\n",
      "Bleu_1: 0.705\n",
      "Bleu_2: 0.531\n",
      "Bleu_3: 0.385\n",
      "Bleu_4: 0.276\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.516\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.868\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [6.256 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.88 s\n",
      "SPICE: 0.169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:44<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 22 has loss 2.680530\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.056180 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1118959.53 tokens per second.\n",
      "PTBTokenizer tokenized 53225 tokens at 420000.64 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48226, 'reflen': 47814, 'guess': [48226, 43226, 38226, 33226], 'correct': [34235, 17432, 7781, 3408]}\n",
      "ratio: 1.0086167231354624\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.535\n",
      "Bleu_3: 0.388\n",
      "Bleu_4: 0.278\n",
      "computing METEOR score...\n",
      "METEOR: 0.241\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.517\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.877\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.989 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 14.55 s\n",
      "SPICE: 0.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:09<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 23 has loss 2.672315\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.326889 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1210572.33 tokens per second.\n",
      "PTBTokenizer tokenized 53312 tokens at 356001.43 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48313, 'reflen': 47869, 'guess': [48313, 43313, 38313, 33313], 'correct': [34204, 17408, 7801, 3407]}\n",
      "ratio: 1.0092753138774362\n",
      "Bleu_1: 0.708\n",
      "Bleu_2: 0.533\n",
      "Bleu_3: 0.387\n",
      "Bleu_4: 0.277\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.517\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.874\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.151 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.89 s\n",
      "SPICE: 0.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:42<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 24 has loss 2.666543\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.992863 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:43<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1186174.15 tokens per second.\n",
      "PTBTokenizer tokenized 53177 tokens at 427401.57 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48178, 'reflen': 47829, 'guess': [48178, 43178, 38178, 33178], 'correct': [34196, 17363, 7764, 3365]}\n",
      "ratio: 1.0072968282840744\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.534\n",
      "Bleu_3: 0.387\n",
      "Bleu_4: 0.277\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.518\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.874\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.52 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.89 s\n",
      "SPICE: 0.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:29<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 25 has loss 2.661587\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 6.919044 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1136679.65 tokens per second.\n",
      "PTBTokenizer tokenized 53261 tokens at 418333.30 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48262, 'reflen': 47830, 'guess': [48262, 43262, 38262, 33262], 'correct': [34268, 17434, 7804, 3408]}\n",
      "ratio: 1.009031988291846\n",
      "Bleu_1: 0.710\n",
      "Bleu_2: 0.535\n",
      "Bleu_3: 0.388\n",
      "Bleu_4: 0.278\n",
      "computing METEOR score...\n",
      "METEOR: 0.241\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.517\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.879\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.633 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.63 s\n",
      "SPICE: 0.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:17<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 26 has loss 2.656744\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 6.866819 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1234997.69 tokens per second.\n",
      "PTBTokenizer tokenized 53308 tokens at 422682.49 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48309, 'reflen': 47841, 'guess': [48309, 43309, 38309, 33309], 'correct': [34180, 17421, 7845, 3464]}\n",
      "ratio: 1.0097824042139376\n",
      "Bleu_1: 0.708\n",
      "Bleu_2: 0.533\n",
      "Bleu_3: 0.388\n",
      "Bleu_4: 0.279\n",
      "computing METEOR score...\n",
      "METEOR: 0.241\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.517\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.876\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.229 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.37 s\n",
      "SPICE: 0.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:08<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 27 has loss 2.651970\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.245970 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1198670.05 tokens per second.\n",
      "PTBTokenizer tokenized 53427 tokens at 395043.90 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48428, 'reflen': 47942, 'guess': [48428, 43428, 38428, 33428], 'correct': [34194, 17295, 7694, 3354]}\n",
      "ratio: 1.0101372491760667\n",
      "Bleu_1: 0.706\n",
      "Bleu_2: 0.530\n",
      "Bleu_3: 0.383\n",
      "Bleu_4: 0.274\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.515\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.868\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.905 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 14.53 s\n",
      "SPICE: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:31<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 28 has loss 2.647081\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 7.401269 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1147036.77 tokens per second.\n",
      "PTBTokenizer tokenized 53295 tokens at 412777.68 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48296, 'reflen': 47911, 'guess': [48296, 43296, 38296, 33296], 'correct': [34200, 17340, 7741, 3371]}\n",
      "ratio: 1.0080357329214376\n",
      "Bleu_1: 0.708\n",
      "Bleu_2: 0.533\n",
      "Bleu_3: 0.386\n",
      "Bleu_4: 0.276\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.516\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.874\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [3.434 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 14.81 s\n",
      "SPICE: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5664 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████| 5664/5664 [43:06<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 29 has loss 2.639408\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 5.802271 secs\n",
      "[DEBUG] Running inference on val with 250 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:46<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307821 tokens at 1153563.14 tokens per second.\n",
      "PTBTokenizer tokenized 53409 tokens at 421763.29 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48410, 'reflen': 47973, 'guess': [48410, 43410, 38410, 33410], 'correct': [34212, 17309, 7738, 3375]}\n",
      "ratio: 1.0091092906426322\n",
      "Bleu_1: 0.707\n",
      "Bleu_2: 0.531\n",
      "Bleu_3: 0.384\n",
      "Bleu_4: 0.275\n",
      "computing METEOR score...\n",
      "METEOR: 0.240\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.516\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.869\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [2.545 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.12 s\n",
      "SPICE: 0.170\n"
     ]
    }
   ],
   "source": [
    "  for epoch in range(30): \n",
    "    loss_train = 0.\n",
    "    \n",
    "    if(epoch == args.finetune_after):\n",
    "        \n",
    "        img_optimizer = optim.RMSprop(model_imgcnn.parameters(), lr=1e-5)\n",
    "        img_scheduler = lr_scheduler.StepLR(img_optimizer, step_size=args.lr_step_size, gamma=.1)\n",
    "\n",
    "    scheduler.step()    \n",
    "    if(img_optimizer):\n",
    "        img_scheduler.step()\n",
    "\n",
    "    #One epoch of train\n",
    "    for batch_idx, (imgs, captions, wordclass, mask, _) in \\\n",
    "      tqdm(enumerate(train_data_loader), total=nbatches):\n",
    "\n",
    "        imgs = imgs.view(batchsize, 3, 224, 224)\n",
    "        wordclass = wordclass.view(batchsize_cap, max_tokens)\n",
    "        mask = mask.view(batchsize_cap, max_tokens)\n",
    "\n",
    "        imgs_v = Variable(imgs).cuda()\n",
    "        wordclass_v = Variable(wordclass).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if(img_optimizer):\n",
    "            img_optimizer.zero_grad() \n",
    "\n",
    "        imgsfeats, imgsfc7 = model_imgcnn(imgs_v)\n",
    "        imgsfeats, imgsfc7 = repeat_img_per_cap(imgsfeats, imgsfc7, ncap_per_img)\n",
    "        _, _, feat_h, feat_w = imgsfeats.size()\n",
    "\n",
    "        if(args.attention == True):\n",
    "            wordact, attn = model_convcap(imgsfeats, imgsfc7, wordclass_v)\n",
    "            attn = attn.view(batchsize_cap, max_tokens, feat_h, feat_w)\n",
    "        else:\n",
    "            wordact, _ = model_convcap(imgsfeats, imgsfc7, wordclass_v)\n",
    "\n",
    "        wordact = wordact[:,:,:-1]\n",
    "        wordclass_v = wordclass_v[:,1:]\n",
    "        mask = mask[:,1:].contiguous()\n",
    "\n",
    "        wordact_t = wordact.permute(0, 2, 1).contiguous().view(\\\n",
    "        batchsize_cap*(max_tokens-1), -1)\n",
    "        wordclass_t = wordclass_v.contiguous().view(\\\n",
    "        batchsize_cap*(max_tokens-1), 1)\n",
    "      \n",
    "        maskids = torch.nonzero(mask.view(-1)).numpy().reshape(-1)\n",
    "\n",
    "        if(args.attention == True):\n",
    "            #Cross-entropy loss and attention loss of Show, Attend and Tell\n",
    "            loss = F.cross_entropy(wordact_t[maskids, ...], \\\n",
    "            wordclass_t[maskids, ...].contiguous().view(maskids.shape[0])) \\\n",
    "            + (torch.sum(torch.pow(1. - torch.sum(attn, 1), 2)))\\\n",
    "            /(batchsize_cap*feat_h*feat_w)\n",
    "        else:\n",
    "            loss = F.cross_entropy(wordact_t[maskids, ...], \\\n",
    "            wordclass_t[maskids, ...].contiguous().view(maskids.shape[0]))\n",
    "\n",
    "        loss_train = loss_train + loss.data\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if(img_optimizer):\n",
    "            img_optimizer.step()\n",
    "\n",
    "    loss_train = (loss_train*1.)/(batch_idx)\n",
    "    print('[DEBUG] Training epoch %d has loss %f' % (epoch, loss_train))\n",
    "\n",
    "    modelfn = osp.join(args.model_dir, 'model.pth')\n",
    "\n",
    "    if(img_optimizer):\n",
    "        img_optimizer_dict = img_optimizer.state_dict()\n",
    "    else:\n",
    "        img_optimizer_dict = None\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model_convcap.state_dict(),\n",
    "        'img_state_dict': model_imgcnn.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        'img_optimizer' : img_optimizer_dict,\n",
    "      }, modelfn)\n",
    "\n",
    "    #Run on validation and obtain score\n",
    "    scores = test(args, 'val', model_convcap=model_convcap, model_imgcnn=model_imgcnn)\n",
    "    score = scores[0][args.score_select]\n",
    "\n",
    "    if(score > bestscore):\n",
    "        bestscore = score\n",
    "        print('[DEBUG] Saving model at epoch %d with %s score of %f'\\\n",
    "        % (epoch, args.score_select, score))\n",
    "        bestmodelfn = osp.join(args.model_dir, 'bestmodel.pth')\n",
    "        os.system('cp %s %s' % (modelfn, bestmodelfn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79704faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 1.78 GiB already allocated; 8.44 MiB free; 1.80 GiB reserved in total by PyTorch)\n",
    "#nvidia-smi #(check PID)\n",
    "#kill -9 PID\n",
    "#import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7331abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfn = osp.join(args.model_dir, 'bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f73dba20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (osp.exists(bestmodelfn)):\n",
      "if (args.beam_size == 1):\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: test\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading test data ... 7.343970 secs\n",
      "[DEBUG] Running inference on test with 250 batches\n",
      "[DEBUG] Loading checkpoint output/bestmodel.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/users/s21110373/convcap-master/convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "/home/users/s21110373/convcap-master/test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|██████████| 250/250 [00:47<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1188416.19 tokens per second.\n",
      "PTBTokenizer tokenized 53061 tokens at 423884.31 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48062, 'reflen': 47728, 'guess': [48062, 43062, 38062, 33062], 'correct': [34158, 17560, 7940, 3536]}\n",
      "ratio: 1.0069979886020575\n",
      "Bleu_1: 0.711\n",
      "Bleu_2: 0.538\n",
      "Bleu_3: 0.392\n",
      "Bleu_4: 0.284\n",
      "computing METEOR score...\n",
      "METEOR: 0.242\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.522\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.897\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Threads( StanfordCoreNLP ) [31.351 seconds]\n",
      "Threads( StanfordCoreNLP ) [28.767 seconds]\n",
      "Threads( StanfordCoreNLP ) [12.471 seconds]\n",
      "Parsing test captions\n",
      "Threads( StanfordCoreNLP ) [4.105 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.534 min\n",
      "SPICE: 0.173\n",
      "TEST set scores\n",
      "Bleu_1: 0.710707\n",
      "Bleu_2: 0.538345\n",
      "Bleu_3: 0.392479\n",
      "Bleu_4: 0.283569\n",
      "METEOR: 0.242125\n",
      "ROUGE_L: 0.521517\n",
      "CIDEr: 0.897134\n",
      "SPICE: 0.172948\n"
     ]
    }
   ],
   "source": [
    "if (osp.exists(bestmodelfn)):\n",
    "    print('if (osp.exists(bestmodelfn)):')\n",
    "    \n",
    "    if (args.beam_size == 1):\n",
    "        print('if (args.beam_size == 1):')\n",
    "        scores = test(args, 'test', modelfn=bestmodelfn)\n",
    "    else:\n",
    "        print('else:')\n",
    "        scores = test_beam(args, 'test', modelfn=bestmodelfn)\n",
    "        \n",
    "    print('TEST set scores')\n",
    "    for k, v in scores[0].items():\n",
    "        print('%s: %f' % (k, v))\n",
    "else:\n",
    "    print('2 else')\n",
    "    raise Exception('No checkpoint found %s' % bestmodelfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f174bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Bleu_1', 0.7107070034538573), ('Bleu_2', 0.5383447164108337), ('Bleu_3', 0.3924791610378962), ('Bleu_4', 0.2835686630269436), ('METEOR', 0.24212451338823512), ('ROUGE_L', 0.5215170103422307), ('CIDEr', 0.8971341132158545), ('SPICE', 0.17294833452678604)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e00f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
